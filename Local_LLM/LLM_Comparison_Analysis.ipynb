{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ LLM ë¹„êµ ë¶„ì„: OpenAI GPT-3.5-turbo vs Llama 3.1 8B\n",
    "\n",
    "## ğŸ“‹ ê°œìš”\n",
    "ì´ notebookì€ Clinical Note ìƒì„±ì—ì„œ OpenAI API (GPT-3.5-turbo)ì™€ Local LLM (Llama 3.1 8B)ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¶„ì„ í•­ëª©:**\n",
    "1. ìƒì„± ì‹œê°„ ë¹„êµ\n",
    "2. Clinical Note í’ˆì§ˆ ë¹„êµ\n",
    "3. ë¹„ìš© ë¶„ì„\n",
    "4. ì¥ë‹¨ì  ë¶„ì„\n",
    "5. ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 1. Ollama ì„¤ì¹˜ ë° Llama 3.1 8B ì¤€ë¹„\n",
    "\n",
    "### 1.1 Ollama ì„¤ì¹˜\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "# 1. https://ollama.com/download/windows ì—ì„œ ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "# 2. ì„¤ì¹˜ í›„ ìë™ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì‹œì‘ë¨\n",
    "```\n",
    "\n",
    "**ì„¤ì¹˜ í™•ì¸:**\n",
    "```bash\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "### 1.2 Llama 3.1 8B ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "```\n",
    "\n",
    "- ëª¨ë¸ í¬ê¸°: ì•½ 4.7GB\n",
    "- ì†Œìš” ì‹œê°„: ì¸í„°ë„· ì†ë„ì— ë”°ë¼ 5-15ë¶„\n",
    "\n",
    "### 1.3 Ollama ì„œë²„ ì‹¤í–‰\n",
    "\n",
    "```bash\n",
    "# ê¸°ë³¸ì ìœ¼ë¡œ ìë™ ì‹¤í–‰ë˜ì§€ë§Œ, ìˆ˜ë™ ì‹¤í–‰ì´ í•„ìš”í•œ ê²½ìš°:\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "- ê¸°ë³¸ í¬íŠ¸: `http://localhost:11434`\n",
    "\n",
    "### 1.4 ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "```bash\n",
    "ollama run llama3.1:8b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "elif platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "else:  # Linux\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 2. ì´ë¡ ì  ë¹„êµ\n",
    "\n",
    "### 2.1 ëª¨ë¸ ìŠ¤í™ ë¹„êµ\n",
    "\n",
    "| í•­ëª© | GPT-3.5-turbo | Llama 3.1 8B |\n",
    "|------|---------------|---------------|\n",
    "| **ê°œë°œì‚¬** | OpenAI | Meta |\n",
    "| **íŒŒë¼ë¯¸í„° ìˆ˜** | 175B (ì¶”ì •) | 8B |\n",
    "| **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´** | 16,384 tokens | 128K tokens |\n",
    "| **ì‹¤í–‰ í™˜ê²½** | í´ë¼ìš°ë“œ API | ë¡œì»¬ (CPU/GPU) |\n",
    "| **ë¹„ìš©** | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ | ë¬´ë£Œ (í•˜ë“œì›¨ì–´ ë¹„ìš©ë§Œ) |\n",
    "| **ì†ë„** | ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ | í•˜ë“œì›¨ì–´ ì˜ì¡´ |\n",
    "| **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ** | ì„œë²„ ì „ì†¡ | ë¡œì»¬ ì²˜ë¦¬ |\n",
    "\n",
    "### 2.2 ë¹„ìš© ë¹„êµ\n",
    "\n",
    "**GPT-3.5-turbo:**\n",
    "- Input: $0.50 / 1M tokens\n",
    "- Output: $1.50 / 1M tokens\n",
    "- ì˜ˆìƒ ë¹„ìš© (í™˜ì 1ëª…, í•œê¸€+ì˜ì–´): ì•½ $0.02\n",
    "\n",
    "**Llama 3.1 8B:**\n",
    "- API ë¹„ìš©: $0 (ë¬´ë£Œ)\n",
    "- ì´ˆê¸° í•˜ë“œì›¨ì–´ ë¹„ìš©: GPU ìˆìœ¼ë©´ ìœ ë¦¬\n",
    "- ì „ê¸° ë¹„ìš©: ë¯¸ë¯¸\n",
    "\n",
    "### 2.3 í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "**Llama 3.1 8B:**\n",
    "- ìµœì†Œ RAM: 8GB\n",
    "- ê¶Œì¥ RAM: 16GB\n",
    "- GPU: ì„ íƒ ì‚¬í•­ (NVIDIA GPU ìˆìœ¼ë©´ 10-20ë°° ë¹ ë¦„)\n",
    "- ë””ìŠ¤í¬: 5GB (ëª¨ë¸ ì €ì¥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 3. Admin Agent ì‹¤í–‰\n",
    "\n",
    "### 3.1 OpenAI API (GPT-3.5-turbo) - ì´ë¯¸ ì‹¤í–‰ ì™„ë£Œ\n",
    "\n",
    "ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: `../Admin_Agent/data/output_notes/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI APIë¡œ ìƒì„±ëœ íŒŒì¼ í™•ì¸\n",
    "openai_output_dir = \"../Admin_Agent/data/output_notes/\"\n",
    "openai_files = os.listdir(openai_output_dir)\n",
    "print(f\"OpenAI APIë¡œ ìƒì„±ëœ íŒŒì¼ ({len(openai_files)}ê°œ):\")\n",
    "for f in openai_files:\n",
    "    file_path = os.path.join(openai_output_dir, f)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"  - {f} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Local LLM (Llama 3.1 8B) ì‹¤í–‰\n",
    "\n",
    "**ì£¼ì˜:** Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama ì„œë²„ ìƒíƒœ í™•ì¸\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434\")\n",
    "    print(\"[OK] Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!\")\n",
    "    print(f\"ì‘ë‹µ: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(\"[FAIL] Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ì˜¤ë¥˜: {str(e)}\")\n",
    "    print(\"\\nëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ 'ollama serve'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.1 8Bë¡œ Clinical Note ìƒì„±\n",
    "# ì£¼ì˜: ì´ ì…€ì„ ì‹¤í–‰í•˜ê¸° ì „ì— Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤!\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"Llama 3.1 8Bë¡œ Clinical Note ìƒì„± ì‹œì‘...\")\n",
    "print(\"ì´ ì‘ì—…ì€ í•˜ë“œì›¨ì–´ì— ë”°ë¼ 5-30ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Admin Agent Local ì‹¤í–‰\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"Admin_Agent_Local/scripts/admin_agent_local.py\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    input=\"\\n\"  # Enter í‚¤ ìë™ ì…ë ¥\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nì‹¤í–‰ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ\")\n",
    "print(\"\\n=== ì¶œë ¥ ===\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\n=== ì˜¤ë¥˜ ===\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamaë¡œ ìƒì„±ëœ íŒŒì¼ í™•ì¸\n",
    "llama_output_dir = \"Admin_Agent_Local/data/output_notes/\"\n",
    "llama_files = os.listdir(llama_output_dir) if os.path.exists(llama_output_dir) else []\n",
    "print(f\"Llama 3.1 8Bë¡œ ìƒì„±ëœ íŒŒì¼ ({len(llama_files)}ê°œ):\")\n",
    "for f in llama_files:\n",
    "    file_path = os.path.join(llama_output_dir, f)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"  - {f} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 4. ì •ëŸ‰ì  ë¹„êµ ë¶„ì„\n",
    "\n",
    "### 4.1 ìƒì„± ì‹œê°„ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„± ì‹œê°„ ë°ì´í„° (ìˆ˜ë™ ì…ë ¥ ë˜ëŠ” ë¡œê·¸ì—ì„œ ì¶”ì¶œ)\n",
    "# ì˜ˆì‹œ ë°ì´í„° - ì‹¤ì œ ì‹¤í–‰ í›„ ì—…ë°ì´íŠ¸ í•„ìš”\n",
    "\n",
    "time_comparison = {\n",
    "    \"ëª¨ë¸\": [\"GPT-3.5-turbo\", \"Llama 3.1 8B\"],\n",
    "    \"í™˜ìë‹¹_í‰ê· _ì‹œê°„(ì´ˆ)\": [5, 20],  # ì‹¤ì œ ì¸¡ì • ê°’ìœ¼ë¡œ êµì²´\n",
    "    \"2ëª…_ì´_ì‹œê°„(ì´ˆ)\": [10, 40]  # ì‹¤ì œ ì¸¡ì • ê°’ìœ¼ë¡œ êµì²´\n",
    "}\n",
    "\n",
    "df_time = pd.DataFrame(time_comparison)\n",
    "print(\"ìƒì„± ì‹œê°„ ë¹„êµ:\")\n",
    "print(df_time)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(df_time[\"ëª¨ë¸\"]))\n",
    "ax.bar(x, df_time[\"í™˜ìë‹¹_í‰ê· _ì‹œê°„(ì´ˆ)\"], color=['#2E86AB', '#A23B72'])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_time[\"ëª¨ë¸\"])\n",
    "ax.set_ylabel(\"ì‹œê°„ (ì´ˆ)\")\n",
    "ax.set_title(\"í™˜ìë‹¹ Clinical Note ìƒì„± ì‹œê°„ ë¹„êµ\")\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(df_time[\"í™˜ìë‹¹_í‰ê· _ì‹œê°„(ì´ˆ)\"]):\n",
    "    ax.text(i, v + 1, f\"{v}ì´ˆ\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"time_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 íŒŒì¼ í¬ê¸° ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¼ í¬ê¸° ë¹„êµ\n",
    "def get_file_sizes(directory):\n",
    "    sizes = []\n",
    "    for f in os.listdir(directory):\n",
    "        if f.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, f)\n",
    "            sizes.append(os.path.getsize(file_path))\n",
    "    return sizes\n",
    "\n",
    "openai_sizes = get_file_sizes(openai_output_dir)\n",
    "llama_sizes = get_file_sizes(llama_output_dir) if os.path.exists(llama_output_dir) else []\n",
    "\n",
    "size_data = {\n",
    "    \"ëª¨ë¸\": [\"GPT-3.5-turbo\"] * len(openai_sizes) + [\"Llama 3.1 8B\"] * len(llama_sizes),\n",
    "    \"í¬ê¸°(bytes)\": openai_sizes + llama_sizes\n",
    "}\n",
    "\n",
    "df_size = pd.DataFrame(size_data)\n",
    "\n",
    "# Box plotìœ¼ë¡œ ë¹„êµ\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df_size.boxplot(column='í¬ê¸°(bytes)', by='ëª¨ë¸', ax=ax)\n",
    "ax.set_title(\"Clinical Note íŒŒì¼ í¬ê¸° ë¶„í¬\")\n",
    "ax.set_xlabel(\"ëª¨ë¸\")\n",
    "ax.set_ylabel(\"íŒŒì¼ í¬ê¸° (bytes)\")\n",
    "plt.suptitle(\"\")  # ê¸°ë³¸ ì œëª© ì œê±°\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"size_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\ní‰ê·  íŒŒì¼ í¬ê¸°:\")\n",
    "print(df_size.groupby('ëª¨ë¸')['í¬ê¸°(bytes)'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ë¹„ìš© ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ìš© ê³„ì‚° (100ëª…ì˜ í™˜ì ê¸°ì¤€)\n",
    "num_patients = 100\n",
    "notes_per_patient = 2  # í•œê¸€ + ì˜ì–´\n",
    "\n",
    "# GPT-3.5-turbo ë¹„ìš© (ì¶”ì •)\n",
    "avg_input_tokens = 500  # í™˜ì ë°ì´í„° í”„ë¡¬í”„íŠ¸\n",
    "avg_output_tokens = 300  # ìƒì„±ëœ Clinical Note\n",
    "gpt_cost_per_patient = (\n",
    "    (avg_input_tokens / 1_000_000) * 0.50 + \n",
    "    (avg_output_tokens / 1_000_000) * 1.50\n",
    ") * notes_per_patient\n",
    "\n",
    "gpt_total_cost = gpt_cost_per_patient * num_patients\n",
    "\n",
    "# Llama 3.1 8B ë¹„ìš©\n",
    "llama_cost = 0  # ë¬´ë£Œ\n",
    "\n",
    "cost_comparison = pd.DataFrame({\n",
    "    \"ëª¨ë¸\": [\"GPT-3.5-turbo\", \"Llama 3.1 8B\"],\n",
    "    f\"{num_patients}ëª…_ì´_ë¹„ìš©($)\": [gpt_total_cost, llama_cost],\n",
    "    \"í™˜ìë‹¹_ë¹„ìš©($)\": [gpt_cost_per_patient, 0]\n",
    "})\n",
    "\n",
    "print(f\"\\n{num_patients}ëª…ì˜ í™˜ì Clinical Note ìƒì„± ë¹„ìš©:\")\n",
    "print(cost_comparison)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(cost_comparison[\"ëª¨ë¸\"]))\n",
    "ax.bar(x, cost_comparison[f\"{num_patients}ëª…_ì´_ë¹„ìš©($)\"], color=['#2E86AB', '#A23B72'])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cost_comparison[\"ëª¨ë¸\"])\n",
    "ax.set_ylabel(\"ë¹„ìš© ($)\")\n",
    "ax.set_title(f\"{num_patients}ëª… í™˜ì Clinical Note ìƒì„± ë¹„ìš© ë¹„êµ\")\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(cost_comparison[f\"{num_patients}ëª…_ì´_ë¹„ìš©($)\"]):\n",
    "    ax.text(i, v + 0.05, f\"${v:.3f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cost_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 5. ì •ì„±ì  ë¹„êµ ë¶„ì„\n",
    "\n",
    "### 5.1 Clinical Note ìƒ˜í”Œ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°™ì€ í™˜ìì— ëŒ€í•œ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ\n",
    "patient_id = \"R-1731-00000001\"\n",
    "\n",
    "# GPT-3.5-turbo ê²°ê³¼\n",
    "gpt_file = [f for f in openai_files if patient_id in f and \"korean\" in f][0]\n",
    "with open(os.path.join(openai_output_dir, gpt_file), 'r', encoding='utf-8') as f:\n",
    "    gpt_note = f.read()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"í™˜ì {patient_id} - GPT-3.5-turbo ìƒì„± Clinical Note\")\n",
    "print(\"=\" * 60)\n",
    "print(gpt_note)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Llama 3.1 8B ê²°ê³¼ (ìƒì„±ëœ ê²½ìš°)\n",
    "if llama_files:\n",
    "    llama_file = [f for f in llama_files if patient_id in f and \"korean\" in f][0]\n",
    "    with open(os.path.join(llama_output_dir, llama_file), 'r', encoding='utf-8') as f:\n",
    "        llama_note = f.read()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"í™˜ì {patient_id} - Llama 3.1 8B ìƒì„± Clinical Note\")\n",
    "    print(\"=\" * 60)\n",
    "    print(llama_note)\n",
    "else:\n",
    "    print(\"Llama 3.1 8B ê²°ê³¼ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 í’ˆì§ˆ í‰ê°€ ê¸°ì¤€\n",
    "\n",
    "Clinical Noteì˜ í’ˆì§ˆì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ì™„ì „ì„±** (Completeness): ëª¨ë“  í•„ìˆ˜ ì„¹ì…˜ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?\n",
    "2. **ì •í™•ì„±** (Accuracy): í™˜ì ë°ì´í„°ê°€ ì •í™•í•˜ê²Œ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?\n",
    "3. **ìì—°ìŠ¤ëŸ¬ì›€** (Fluency): ë¬¸ì¥ì´ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ê°€?\n",
    "4. **ì „ë¬¸ì„±** (Professionalism): ì˜í•™ ìš©ì–´ ì‚¬ìš©ì´ ì ì ˆí•œê°€?\n",
    "5. **ì¼ê´€ì„±** (Consistency): ì •ë³´ ê°„ ëª¨ìˆœì´ ì—†ëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™ í‰ê°€ í›„ ì…ë ¥)\n",
    "# ê° í•­ëª©ì„ 1-5ì ìœ¼ë¡œ í‰ê°€\n",
    "\n",
    "quality_scores = {\n",
    "    \"í‰ê°€ í•­ëª©\": [\"ì™„ì „ì„±\", \"ì •í™•ì„±\", \"ìì—°ìŠ¤ëŸ¬ì›€\", \"ì „ë¬¸ì„±\", \"ì¼ê´€ì„±\"],\n",
    "    \"GPT-3.5-turbo\": [5, 5, 5, 4, 5],  # ì‹¤ì œ í‰ê°€ í›„ ì—…ë°ì´íŠ¸\n",
    "    \"Llama 3.1 8B\": [4, 4, 4, 3, 4]  # ì‹¤ì œ í‰ê°€ í›„ ì—…ë°ì´íŠ¸\n",
    "}\n",
    "\n",
    "df_quality = pd.DataFrame(quality_scores)\n",
    "df_quality.set_index(\"í‰ê°€ í•­ëª©\", inplace=True)\n",
    "\n",
    "print(\"\\ní’ˆì§ˆ í‰ê°€ ì ìˆ˜ (1-5ì ):\")\n",
    "print(df_quality)\n",
    "\n",
    "# ë ˆì´ë” ì°¨íŠ¸ë¡œ ì‹œê°í™”\n",
    "import numpy as np\n",
    "\n",
    "categories = df_quality.index.tolist()\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# GPT-3.5-turbo\n",
    "values = df_quality[\"GPT-3.5-turbo\"].tolist()\n",
    "values += values[:1]\n",
    "ax.plot(angles, values, 'o-', linewidth=2, label=\"GPT-3.5-turbo\", color='#2E86AB')\n",
    "ax.fill(angles, values, alpha=0.25, color='#2E86AB')\n",
    "\n",
    "# Llama 3.1 8B\n",
    "values = df_quality[\"Llama 3.1 8B\"].tolist()\n",
    "values += values[:1]\n",
    "ax.plot(angles, values, 'o-', linewidth=2, label=\"Llama 3.1 8B\", color='#A23B72')\n",
    "ax.fill(angles, values, alpha=0.25, color='#A23B72')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_yticks([1, 2, 3, 4, 5])\n",
    "ax.set_yticklabels(['1', '2', '3', '4', '5'])\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.set_title(\"Clinical Note í’ˆì§ˆ í‰ê°€\", pad=20, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"quality_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6. ì¢…í•© ë¹„êµ ë¶„ì„\n",
    "\n",
    "### 6.1 ì¥ë‹¨ì  ë¹„êµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5-turbo (OpenAI API)\n",
    "\n",
    "**ì¥ì :**\n",
    "- âœ… ë†’ì€ í’ˆì§ˆ: ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "- âœ… ë¹ ë¥¸ ì†ë„: ë„¤íŠ¸ì›Œí¬ë§Œ ì–‘í˜¸í•˜ë©´ ë¹ ë¥¸ ì‘ë‹µ\n",
    "- âœ… ì„¤ì • ê°„ë‹¨: API í‚¤ë§Œ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥\n",
    "- âœ… ì•ˆì •ì„±: OpenAIì˜ ì¸í”„ë¼ë¡œ ë†’ì€ ê°€ìš©ì„±\n",
    "- âœ… ì§€ì†ì  ê°œì„ : ëª¨ë¸ì´ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨\n",
    "\n",
    "**ë‹¨ì :**\n",
    "- âŒ ë¹„ìš© ë°œìƒ: ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ê³¼ê¸ˆ\n",
    "- âŒ ë°ì´í„° í”„ë¼ì´ë²„ì‹œ: í™˜ì ë°ì´í„°ê°€ ì™¸ë¶€ ì„œë²„ë¡œ ì „ì†¡\n",
    "- âŒ ì¸í„°ë„· í•„ìˆ˜: ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‚¬ìš© ë¶ˆê°€\n",
    "- âŒ API ì œí•œ: Rate limit, í• ë‹¹ëŸ‰ ì œí•œ\n",
    "- âŒ ì¢…ì†ì„±: OpenAI ì„œë¹„ìŠ¤ì— ì˜ì¡´\n",
    "\n",
    "### Llama 3.1 8B (Local LLM)\n",
    "\n",
    "**ì¥ì :**\n",
    "- âœ… ë¬´ë£Œ: API ë¹„ìš© ì—†ìŒ\n",
    "- âœ… ë°ì´í„° í”„ë¼ì´ë²„ì‹œ: ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬\n",
    "- âœ… ì˜¤í”„ë¼ì¸ ê°€ëŠ¥: ì¸í„°ë„· ì—†ì´ ì‚¬ìš© ê°€ëŠ¥\n",
    "- âœ… ì œí•œ ì—†ìŒ: Rate limit ì—†ìŒ\n",
    "- âœ… ì»¤ìŠ¤í„°ë§ˆì´ì§•: Fine-tuning ê°€ëŠ¥\n",
    "\n",
    "**ë‹¨ì :**\n",
    "- âŒ í’ˆì§ˆ: GPT-3.5ë³´ë‹¤ ë‚®ì„ ìˆ˜ ìˆìŒ (íŠ¹íˆ í•œê¸€)\n",
    "- âŒ ëŠë¦° ì†ë„: CPUë§Œ ìˆìœ¼ë©´ ë§¤ìš° ëŠë¦¼\n",
    "- âŒ í•˜ë“œì›¨ì–´ ìš”êµ¬: RAM 8GB ì´ìƒ í•„ìš”\n",
    "- âŒ ì„¤ì • ë³µì¡: Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”\n",
    "- âŒ ì—…ë°ì´íŠ¸: ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸ í•„ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ì‹œë‚˜ë¦¬ì˜¤ | ì¶”ì²œ ëª¨ë¸ | ì´ìœ  |\n",
    "|---------|----------|------|\n",
    "| **í”„ë¡œí† íƒ€ì… ê°œë°œ** | GPT-3.5-turbo | ë¹ ë¥¸ ì„¤ì •, ë†’ì€ í’ˆì§ˆ |\n",
    "| **ì†Œê·œëª¨ ì—°êµ¬ (<100ëª…)** | GPT-3.5-turbo | ë¹„ìš©ì´ ì ê³  í’ˆì§ˆì´ ì¤‘ìš” |\n",
    "| **ëŒ€ê·œëª¨ ì—°êµ¬ (>1000ëª…)** | Llama 3.1 8B | ë¹„ìš© ì ˆê° (ë¬´ë£Œ) |\n",
    "| **ë¯¼ê°í•œ í™˜ì ë°ì´í„°** | Llama 3.1 8B | ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´ì¥ |\n",
    "| **ì˜¤í”„ë¼ì¸ í™˜ê²½** | Llama 3.1 8B | ì¸í„°ë„· ë¶ˆí•„ìš” |\n",
    "| **ìµœê³  í’ˆì§ˆ í•„ìš”** | GPT-3.5-turbo | ë” ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì  |\n",
    "| **GPU ì„œë²„ ë³´ìœ ** | Llama 3.1 8B | ì†ë„ í–¥ìƒ + ë¬´ë£Œ |\n",
    "| **ë¹ ë¥¸ ê²°ê³¼ í•„ìš”** | GPT-3.5-turbo | í´ë¼ìš°ë“œ ì†ë„ |\n",
    "| **Fine-tuning í•„ìš”** | Llama 3.1 8B | ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 7. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "### 7.1 í•µì‹¬ ë°œê²¬\n",
    "\n",
    "1. **í’ˆì§ˆ**: GPT-3.5-turboê°€ ì¼ë°˜ì ìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ Clinical Note ìƒì„±\n",
    "2. **ë¹„ìš©**: Llama 3.1 8BëŠ” ë¬´ë£Œì´ë©°, ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹œ í° ë¹„ìš© ì ˆê°\n",
    "3. **ì†ë„**: í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ GPT-3.5-turboê°€ ë” ë¹ ë¦„\n",
    "4. **í”„ë¼ì´ë²„ì‹œ**: Llama 3.1 8Bê°€ ë¡œì»¬ ì²˜ë¦¬ë¡œ ë°ì´í„° ë³´ì•ˆ ìš°ìˆ˜\n",
    "\n",
    "### 7.2 Multi-Agent System ì ìš© ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "**í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼:**\n",
    "```\n",
    "Formatter Agent (ë¡œì»¬) â†’ ED Physician Agent (API) â†’ Specialist Agent (API)\n",
    "       â†“                           â†“                         â†“\n",
    "  Llama 3.1 8B            GPT-3.5-turbo            GPT-4 (ê³ ê¸‰)\n",
    "```\n",
    "\n",
    "- **Formatter Agent**: Llama 3.1 8B (ê°„ë‹¨í•œ ë°ì´í„° ì •ë¦¬, ë¬´ë£Œ)\n",
    "- **ED Physician**: GPT-3.5-turbo (ë¹ ë¥´ê³  í’ˆì§ˆ ì¢‹ìŒ)\n",
    "- **Specialist**: GPT-4 (ìµœê³  í’ˆì§ˆ í•„ìš”ì‹œ)\n",
    "\n",
    "### 7.3 í–¥í›„ ì—°êµ¬ ë°©í–¥\n",
    "\n",
    "1. **Fine-tuning**: Llama 3.1 8Bë¥¼ ì˜ë£Œ ë°ì´í„°ë¡œ Fine-tuningí•˜ì—¬ í’ˆì§ˆ í–¥ìƒ\n",
    "2. **ì•™ìƒë¸”**: ë‘ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìµœì í™”\n",
    "3. **ë²¤ì¹˜ë§ˆí‚¹**: ë” ë§ì€ í™˜ì ë°ì´í„°ë¡œ ê´‘ë²”ìœ„í•œ í‰ê°€\n",
    "4. **ë‹¤êµ­ì–´**: ì˜ì–´ ì™¸ ë‹¤ë¥¸ ì–¸ì–´ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Ollama ê³µì‹ ë¬¸ì„œ](https://ollama.com/docs)\n",
    "- [Llama 3.1 ëª¨ë¸ ì¹´ë“œ](https://ollama.com/library/llama3.1)\n",
    "- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs)\n",
    "- [AutoGen ë¬¸ì„œ](https://microsoft.github.io/autogen/)\n",
    "\n",
    "---\n",
    "\n",
    "**ë¶„ì„ ì‘ì„±ì¼**: 2025-01-15  \n",
    "**ë¶„ì„ì**: Multi-Agent EME í”„ë¡œì íŠ¸ íŒ€  \n",
    "**ë²„ì „**: 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
